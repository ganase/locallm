import os
import json
import math
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# ---------------------------------------------------------
# .env èª­ã¿è¾¼ã¿ & ç’°å¢ƒå¤‰æ•°
# ---------------------------------------------------------
load_dotenv()

# ãƒãƒ£ãƒƒãƒˆç”¨ LLM
LLM_API_KEY = os.getenv("LOCALLM_API_KEY") or os.getenv("LLM_API_KEY")
LLM_BASE_URL = os.getenv("LOCALLM_BASE_URL") or os.getenv(
    "LLM_BASE_URL",
    "",
)
LLM_MODEL = os.getenv("LOCALLM_CHAT_MODEL") or os.getenv("LLM_MODEL", "")

# åŸ‹ã‚è¾¼ã¿ç”¨ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ã«ä¾å­˜ã—ãªã„æŠ½è±¡åï¼‰
EMB_API_KEY = os.getenv("EMB_API_KEY")
EMB_BASE_URL = os.getenv("EMB_BASE_URL", "https://api.openai.com/v1")
EMB_MODEL = os.getenv("EMB_MODEL", "text-embedding-3-small")

# ---------------------------------------------------------
# ãƒ‘ã‚¹è¨­å®š
#   Locallm/
#     app/app_emb.py
#     data/knowledge.txt
#     data/system_prompt.txt
#     data/uploads/
#     logs/
# ---------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = BASE_DIR / "data"
LOGS_DIR = BASE_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

UPLOAD_DIR = DATA_DIR / "uploads"
UPLOAD_DIR.mkdir(exist_ok=True)


# ---------------------------------------------------------
# ãƒ­ã‚°æ›¸ãè¾¼ã¿ï¼ˆ1è¡Œ1JSON ã® jsonl å½¢å¼ï¼‰
# ---------------------------------------------------------
def log_interaction(
    question: str,
    answer: str,
    contexts: List[str],
    extra: Dict[str, Any] | None = None,
) -> None:
    """logs/YYYYMMDD.jsonl ã« Q&A ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½è¨˜"""
    extra = extra or {}
    date_str = datetime.now().strftime("%Y%m%d")
    log_path = LOGS_DIR / f"{date_str}.jsonl"

    record: Dict[str, Any] = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer,
        "contexts": contexts,
    }
    record.update(extra)

    try:
        with log_path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    except Exception:
        # ãƒ­ã‚°æ›¸ãè¾¼ã¿å¤±æ•—ã¯ç„¡è¦–ï¼ˆã‚¢ãƒ—ãƒªãŒè½ã¡ãªã„ã‚ˆã†ã«ï¼‰
        pass


def list_log_files() -> List[Path]:
    """logs/ é…ä¸‹ã® *.jsonl ã‚’æ–°ã—ã„é †ã«è¿”ã™"""
    files = sorted(LOGS_DIR.glob("*.jsonl"), reverse=True)
    return files


def load_history_from_log(log_path: Path) -> List[Dict[str, str]]:
    """logs/YYYYMMDD_xxxxxx.jsonl ã‹ã‚‰ history ã‚’çµ„ã¿ç«‹ã¦ã‚‹"""
    history: List[Dict[str, str]] = []
    if not log_path.exists():
        return history

    with log_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue
            q = rec.get("question")
            a = rec.get("answer")
            if q and a:
                history.append({"user": q, "assistant": a})
    return history


# ---------------------------------------------------------
# LLM / Embedding ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ---------------------------------------------------------
def get_llm_client():
    """ãƒãƒ£ãƒƒãƒˆç”¨ LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    if not LLM_API_KEY:
        return "LLM_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    client = OpenAI(
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL,
    )
    return client


def get_emb_client():
    """Embedding ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆOpenAI / Azure / ãã®ä»– ä½•ã§ã‚‚å¯ï¼‰"""
    if not EMB_API_KEY:
        return "EMB_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    client = OpenAI(
        api_key=EMB_API_KEY,
        base_url=EMB_BASE_URL,
    )
    return client


# ---------------------------------------------------------
# system_prompt.txt èª­ã¿è¾¼ã¿
# ---------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_system_prompt() -> str:
    """
    data/system_prompt.txt ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€ã€‚
    ç„¡ã„ or ç©ºãªã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™ã€‚
    """
    path = DATA_DIR / "system_prompt.txt"
    if path.exists():
        txt = path.read_text(encoding="utf-8", errors="ignore").strip()
        if txt:
            return txt

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
    return (
        "ã‚ãªãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ´»ç”¨ã™ã‚‹ç¤¾å†…ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯AIã§ã™ã€‚"
        "å¸¸ã«æ—¥æœ¬èªã§ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ãŒã‚ã‚Œã°ã§ãã‚‹ã ã‘å„ªå…ˆã—ã¦æ´»ç”¨ã—ã€"
        "ãƒŠãƒ¬ãƒƒã‚¸ã«ç„¡ã„å†…å®¹ã«ã¤ã„ã¦èã‹ã‚ŒãŸå ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆãŸä¸Šã§ã€"
        "ä¸€èˆ¬è«–ã¨ã—ã¦ç­”ãˆã‚‰ã‚Œã‚‹ç¯„å›²ã§è£œè¶³ã—ã¦ãã ã•ã„ã€‚"
    )


# ---------------------------------------------------------
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸èª­ã¿è¾¼ã¿
#   - data/knowledge.txt ï¼ˆç©ºè¡ŒåŒºåˆ‡ã‚Šã§ 1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
#   - data/uploads/*.txt, *.md ï¼ˆç©ºè¡ŒåŒºåˆ‡ã‚Šã§ 1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
#   - data/uploads/*.csv ï¼ˆ1 è¡Œ = 1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ‰±ã„ï¼‰
# ---------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_knowledge() -> List[str]:
    docs: List[str] = []

    # 1) data/knowledge.txt
    knowledge_path = DATA_DIR / "knowledge.txt"
    if knowledge_path.exists():
        text = knowledge_path.read_text(encoding="utf-8", errors="ignore")
        docs.extend(b.strip() for b in text.split("\n\n") if b.strip())

    # 2) data/uploads/*.txt, *.md
    for path in UPLOAD_DIR.glob("*.txt"):
        try:
            text = path.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        docs.extend(b.strip() for b in text.split("\n\n") if b.strip())

    # 3) data/uploads/*.csv ï½¥ï½¥ï½¥ 1 è¡Œ = 1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    import csv

    for path in UPLOAD_DIR.glob("*.csv"):
        try:
            with path.open("r", encoding="utf-8", errors="ignore", newline="") as f:
                reader = csv.reader(f)
                _header = next(reader, None)
                for row in reader:
                    line = ", ".join(col.strip() for col in row if col.strip())
                    if line:
                        docs.append(line)
        except Exception:
            continue

    return docs


def get_knowledge_docs() -> List[str]:
    return load_knowledge()


# ---------------------------------------------------------
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
# ---------------------------------------------------------
def init_session_state() -> None:
    if "messages" not in st.session_state:
        st.session_state.messages: List[Dict[str, str]] = []

    if "history" not in st.session_state:
        st.session_state.history: List[Dict[str, str]] = []

    if "loaded_log_name" not in st.session_state:
        st.session_state.loaded_log_name: str | None = None


def add_history(user: str, assistant: str) -> None:
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ & Chat UI ä¸¡æ–¹ã«è¿½åŠ """
    st.session_state.history.append({"user": user, "assistant": assistant})
    st.session_state.messages.append({"role": "user", "content": user})
    st.session_state.messages.append({"role": "assistant", "content": assistant})


def get_history() -> List[Dict[str, str]]:
    return st.session_state.history


# ---------------------------------------------------------
# Embedding é–¢é€£ï¼šã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
# ---------------------------------------------------------
def cosine_similarity(vec_a: List[float], vec_b: List[float]) -> float:
    """å˜ç´”ãªã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ï¼ˆnumpy ã‚’ä½¿ã‚ãªã„ç‰ˆï¼‰"""
    if not vec_a or not vec_b:
        return 0.0

    # é•·ã•ãŒé•ã†å ´åˆã¯çŸ­ã„æ–¹ã«åˆã‚ã›ã‚‹
    n = min(len(vec_a), len(vec_b))
    dot = 0.0
    na = 0.0
    nb = 0.0
    for i in range(n):
        a = vec_a[i]
        b = vec_b[i]
        dot += a * b
        na += a * a
        nb += b * b

    if na == 0.0 or nb == 0.0:
        return 0.0

    return dot / (math.sqrt(na) * math.sqrt(nb))


def embed_texts(texts: List[str]) -> List[List[float]]:
    """ä¸ãˆã‚‰ã‚ŒãŸ texts ã‚’ Embedding ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
    client = get_emb_client()
    if isinstance(client, str):
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆã¯ä¾‹å¤–ã«ã—ã¦ä¸Šä½ã§ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        raise RuntimeError(client)

    resp = client.embeddings.create(
        model=EMB_MODEL,
        input=texts,
    )
    vectors: List[List[float]] = [d.embedding for d in resp.data]
    return vectors


# ---------------------------------------------------------
# ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ– & ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
# ---------------------------------------------------------
@st.cache_resource(show_spinner=True)
def build_corpus_index() -> Tuple[List[str], List[List[float]]]:
    """
    knowledge.txt + uploads ã‚’ã¾ã¨ã‚ã¦èª­ã¿è¾¼ã¿ã€
    åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¦ä¿æŒã™ã‚‹ã€‚
    """
    docs = get_knowledge_docs()
    if not docs:
        return [], []

    vectors = embed_texts(docs)
    return docs, vectors


def retrieve_with_embedding(query: str, top_k: int = 3) -> List[str]:
    """
    ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®é«˜ã„é †ã« top_k ä»¶è¿”ã™
    """
    docs, vectors = build_corpus_index()
    if not docs or not vectors:
        return []

    # ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿
    q_vec = embed_texts([query])[0]

    scored: List[Tuple[float, str]] = []
    for doc, vec in zip(docs, vectors):
        score = cosine_similarity(q_vec, vec)
        if score > 0.0:
            scored.append((score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [doc for score, doc in scored[:top_k]]


# ---------------------------------------------------------
# LLM å‘¼ã³å‡ºã—
# ---------------------------------------------------------
def call_llm_with_context(query: str, contexts: List[str]) -> str:
    client = get_llm_client()
    if isinstance(client, str):
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¿”ã£ã¦ããŸå ´åˆ
        return client

    history = get_history()

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
    if contexts:
        context_text = "\n\n---\n\n".join(contexts)
    else:
        context_text = "ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã‹ã‚‰é–¢é€£æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    base_system_prompt = load_system_prompt()
    system_content = (
        f"{base_system_prompt}\n\n"
        "-----\n"
        "ä»¥ä¸‹ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆknowledge.txt / uploadsï¼‰ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸé–¢é€£æƒ…å ±ã§ã™ã€‚"
        "å¿…è¦ã«å¿œã˜ã¦å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"{context_text}"
    )

    messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_content},
    ]

    # ç›´è¿‘ 5 ã‚¿ãƒ¼ãƒ³åˆ†ã®å±¥æ­´ã‚’è¿½åŠ 
    for turn in history[-5:]:
        messages.append({"role": "user", "content": turn["user"]})
        messages.append({"role": "assistant", "content": turn["assistant"]})

    messages.append({"role": "user", "content": query})

    resp = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
        temperature=0.3,
    )

    answer = resp.choices[0].message.content or ""
    return answer


# ---------------------------------------------------------
# Streamlit UI
# ---------------------------------------------------------
def main() -> None:
    st.set_page_config(
        page_title="Locallm Embeddingç‰ˆ",
        page_icon="ğŸ§ ",
        layout="wide",
    )
    st.title("Locallm Embeddingç‰ˆ (ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢) ğŸ§ ")
    st.caption("ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ + Embedding ã«ã‚ˆã‚‹ RAG ãƒ†ã‚¹ãƒˆç”¨ã‚¢ãƒ—ãƒª")

    init_session_state()
    docs = get_knowledge_docs()
    doc_count = len(docs)

    # -----------------------------
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    # -----------------------------
    with st.sidebar:
        # æ–°è¦ãƒãƒ£ãƒƒãƒˆ
        if st.button("æ–°è¦ãƒãƒ£ãƒƒãƒˆ", use_container_width=True):
            st.session_state.history = []
            st.session_state.messages = []
            st.session_state.loaded_log_name = None
            st.success("æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
            st.rerun()

        st.markdown("---")

        # ãƒ­ã‚°å±¥æ­´
        st.subheader("å±¥æ­´")
        log_files = list_log_files()
        if not log_files:
            st.caption("logs ãƒ•ã‚©ãƒ«ãƒ€ã«ã¾ã ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.caption("ç›´è¿‘ 20 ä»¶")
            for log_path in log_files[:20]:
                label = log_path.stem
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write(label)
                with col2:
                    if st.button("â†’", key=f"load_log_{label}"):
                        history = load_history_from_log(log_path)
                        st.session_state.history = history
                        st.session_state.messages = []
                        for turn in history:
                            st.session_state.messages.append(
                                {"role": "user", "content": turn["user"]}
                            )
                            st.session_state.messages.append(
                                {"role": "assistant", "content": turn["assistant"]}
                            )
                        st.session_state.loaded_log_name = label
                        st.success(f"{label} ã®å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                        st.rerun()

        if st.session_state.loaded_log_name:
            st.info(f"èª­ã¿è¾¼ã¿ä¸­ã®ãƒ­ã‚°: {st.session_state.loaded_log_name}")

        st.markdown("---")

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸æ¦‚è¦
        st.header("ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸")
        st.write(f"knowledge.txt + uploads ã®æ–‡æ›¸æ•°: **{doc_count}** ä»¶")

        knowledge_path = DATA_DIR / "knowledge.txt"
        st.caption("knowledge.txt Path")
        st.code(str(knowledge_path), language="text")
        if knowledge_path.exists() and doc_count > 0:
            st.caption("knowledge.txt ç”±æ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸€ä¾‹ï¼ˆå†’é ­100æ–‡å­—ï¼‰")
            st.write(docs[0][:100])

        st.markdown("---")

        system_prompt_path = DATA_DIR / "system_prompt.txt"
        st.caption("system_prompt.txt Path")
        st.code(str(system_prompt_path), language="text")
        if system_prompt_path.exists():
            try:
                sp_text = system_prompt_path.read_text(encoding="utf-8").strip()
                if sp_text:
                    st.caption("system_prompt.txt å†’é ­100æ–‡å­—")
                    st.write(sp_text[:100])
                else:
                    st.caption("system_prompt.txt ã¯ç©ºã§ã™ã€‚")
            except Exception as e:
                st.caption(f"system_prompt.txt ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        else:
            st.caption("system_prompt.txt ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

        st.markdown("---")
        st.subheader("ç’°å¢ƒæƒ…å ±")
        st.write(f"[LLM] Base URL: `{LLM_BASE_URL}`")
        st.write(f"[LLM] Model    : `{LLM_MODEL}`")
        st.write(f"[EMB] Base URL: `{EMB_BASE_URL}`")
        st.write(f"[EMB] Model    : `{EMB_MODEL}`")

    # -----------------------------
    # ã“ã‚Œã¾ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    # -----------------------------
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # -----------------------------
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    # -----------------------------
    query = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ + Embedding ã§æ¤œç´¢ï¼‰")

    if query:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›è¡¨ç¤º
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.write(query)

        # RAG: Embedding æ¤œç´¢
        with st.spinner("ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆEmbeddingï¼‰ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™..."):
            try:
                contexts = retrieve_with_embedding(query, top_k=3)
            except Exception as e:
                contexts = []
                st.error(f"Embedding æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # LLM å‘¼ã³å‡ºã—
        with st.spinner("LLM ã«å•ã„åˆã‚ã›ä¸­..."):
            answer = call_llm_with_context(query, contexts)

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå›ç­”è¡¨ç¤º
        with st.chat_message("assistant"):
            st.write(answer)

            if contexts:
                with st.expander("ä»Šå›å‚ç…§ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆEmbedding æ¤œç´¢çµæœï¼‰"):
                    for i, ctx in enumerate(contexts, start=1):
                        st.markdown(f"**Doc {i}**")
                        st.write(ctx)
            else:
                st.caption("Embedding ã«ã‚ˆã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ & ãƒ­ã‚°ä¿å­˜
        add_history(query, answer)
        log_interaction(query, answer, contexts)


if __name__ == "__main__":
    main()
