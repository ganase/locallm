import os
import json
import math
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# ---------------------------------------------------------
# .env èª­ã¿è¾¼ã¿ & ç’°å¢ƒå¤‰æ•°
# ---------------------------------------------------------
load_dotenv()

# ãƒãƒ£ãƒƒãƒˆç”¨ LLM (Locallm å´)
LOCALLM_API_KEY = os.getenv("LOCALLM_API_KEY")
LOCALLM_BASE_URL = os.getenv("LOCALLM_BASE_URL")
LOCALLM_CHAT_MODEL = os.getenv("LOCALLM_CHAT_MODEL")

# åŸ‹ã‚è¾¼ã¿ç”¨ãƒ¢ãƒ‡ãƒ«
LOCALLM_EMBEDDING_MODEL = os.getenv("LOCALLM_EMBEDDING_MODEL")

# åŸ‹ã‚è¾¼ã¿å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰
EMB_API_KEY = os.getenv("EMB_API_KEY") or LOCALLM_API_KEY
EMB_BASE_URL = os.getenv("EMB_BASE_URL") or LOCALLM_BASE_URL

# ---------------------------------------------------------
# ãƒ‘ã‚¹è¨­å®š
# app_emb.py ã¯ app/ é…ä¸‹ã«ã‚ã‚‹æƒ³å®š
# ãƒ«ãƒ¼ãƒˆ:
#   Locallm/
#     app/app_emb.py
#     data/knowledge.txt
#     data/system_prompt.txt
#     logs/
# ---------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = BASE_DIR / "data"
LOGS_DIR = BASE_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ ID ã‚’æ—¥ä»˜ + ãƒ©ãƒ³ãƒ€ãƒ ã§ç”Ÿæˆ
def get_session_id() -> str:
    if "session_id" not in st.session_state:
        date_str = datetime.now().strftime("%Y%m%d")
        rand = uuid.uuid4().hex[:8]
        st.session_state.session_id = f"{date_str}_{rand}"
    return st.session_state.session_id


# ---------------------------------------------------------
# ãƒ­ã‚°æ›¸ãè¾¼ã¿ï¼ˆ1è¡Œ1JSON ã® jsonl å½¢å¼ï¼‰
#   ãƒ•ã‚¡ã‚¤ãƒ«å: logs/<session_id>.jsonl
# ---------------------------------------------------------
def log_interaction(
    question: str,
    answer: str,
    contexts: List[str],
    extra: Dict[str, Any] | None = None,
) -> None:
    """logs/<session_id>.jsonl ã« Q&A ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½è¨˜"""
    extra = extra or {}
    session_id = get_session_id()
    log_path = LOGS_DIR / f"{session_id}.jsonl"

    record: Dict[str, Any] = {
        "timestamp": datetime.now().isoformat(),
        "session_id": session_id,
        "question": question,
        "answer": answer,
        "contexts": contexts,
    }
    record.update(extra)

    with log_path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


def list_log_files() -> List[Path]:
    """logs/ é…ä¸‹ã® *.jsonl ã‚’æ–°ã—ã„é †ã«è¿”ã™"""
    files = sorted(LOGS_DIR.glob("*.jsonl"), reverse=True)
    return files


def load_history_from_log(log_path: Path) -> List[Dict[str, str]]:
    """logs/<session>.jsonl ã‹ã‚‰ history ã‚’çµ„ã¿ç«‹ã¦ã‚‹"""
    history: List[Dict[str, str]] = []
    if not log_path.exists():
        return history

    with log_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue
            q = rec.get("question")
            a = rec.get("answer")
            if q and a:
                history.append({"user": q, "assistant": a})
    return history


# ---------------------------------------------------------
# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”Ÿæˆ
# ---------------------------------------------------------
def get_chat_client():
    """ãƒãƒ£ãƒƒãƒˆç”¨ LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    if not LOCALLM_API_KEY:
        return "LOCALLM_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    if not LOCALLM_BASE_URL:
        return "LOCALLM_BASE_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    if not LOCALLM_CHAT_MODEL:
        return "LOCALLM_CHAT_MODEL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    client = OpenAI(
        api_key=LOCALLM_API_KEY,
        base_url=LOCALLM_BASE_URL,
    )
    return client


def get_embedding_client():
    """åŸ‹ã‚è¾¼ã¿ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    - EMB_API_KEY / EMB_BASE_URL ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ
    - ãªã‘ã‚Œã° LOCALLM_* ã‚’åˆ©ç”¨ï¼ˆåŒã˜ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§åŸ‹ã‚è¾¼ã¿ã‚’å–ã‚‹ï¼‰
    """
    if not EMB_API_KEY:
        return "EMB_API_KEY ã‚‚ã—ãã¯ LOCALLM_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    # EMB_BASE_URL ãŒç©ºã§ LOCALLM_BASE_URL ã‚‚ç©ºãªå ´åˆã¯ OpenAI ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«å€’ã™
    base_url = EMB_BASE_URL or "https://api.openai.com/v1"

    if not LOCALLM_EMBEDDING_MODEL:
        return "LOCALLM_EMBEDDING_MODEL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åï¼‰ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    client = OpenAI(
        api_key=EMB_API_KEY,
        base_url=base_url,
    )
    return client


# ---------------------------------------------------------
# system_prompt.txt èª­ã¿è¾¼ã¿
# ---------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_system_prompt() -> str:
    """
    data/system_prompt.txt ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€ã€‚
    ç„¡ã„ or ç©ºãªã‚‰ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™ã€‚
    """
    path = DATA_DIR / "system_prompt.txt"
    if path.exists():
        txt = path.read_text(encoding="utf-8").strip()
        if txt:
            return txt

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    return (
        "ã‚ãªãŸã¯ç¤¾å†…ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯å‘ã‘ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å¸¸ã«æ—¥æœ¬èªã§ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        "æ¬¡ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ãŒã‚ã‚Œã°ã€ã§ãã‚‹ã ã‘å„ªå…ˆã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒŠãƒ¬ãƒƒã‚¸ã«ç„¡ã„å†…å®¹ã«ã¤ã„ã¦èã‹ã‚ŒãŸå ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆãŸä¸Šã§ã€"
        "ä¸€èˆ¬è«–ã¨ã—ã¦ç­”ãˆã‚‰ã‚Œã‚‹ç¯„å›²ã§è£œè¶³ã—ã¦ãã ã•ã„ã€‚"
    )


# ---------------------------------------------------------
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸èª­ã¿è¾¼ã¿ (data/knowledge.txt)
#   ç©ºè¡Œã§åŒºåˆ‡ã£ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå˜ä½ã«åˆ†å‰²
# ---------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_knowledge() -> List[str]:
    path = DATA_DIR / "knowledge.txt"
    if not path.exists():
        return []

    text = path.read_text(encoding="utf-8", errors="ignore")
    blocks = [b.strip() for b in text.split("\n\n") if b.strip()]
    return blocks


def get_knowledge_docs() -> List[str]:
    return load_knowledge()


# ---------------------------------------------------------
# Streamlit ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
# ---------------------------------------------------------
def init_session_state() -> None:
    if "messages" not in st.session_state:
        st.session_state.messages: List[Dict[str, str]] = []

    if "history" not in st.session_state:
        # [{"user": "...", "assistant": "..."}, ...]
        st.session_state.history: List[Dict[str, str]] = []

    if "loaded_log_name" not in st.session_state:
        st.session_state.loaded_log_name: str | None = None

    # session_id ã¯ get_session_id() å´ã§åˆæœŸåŒ–


def add_history(user: str, assistant: str) -> None:
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ & Chat UI ä¸¡æ–¹ã«è¿½åŠ """
    st.session_state.history.append({"user": user, "assistant": assistant})
    st.session_state.messages.append({"role": "user", "content": user})
    st.session_state.messages.append({"role": "assistant", "content": assistant})


def get_history() -> List[Dict[str, str]]:
    return st.session_state.history


# ---------------------------------------------------------
# åŸ‹ã‚è¾¼ã¿ & ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦æ¤œç´¢
# ---------------------------------------------------------
def embed_texts(texts: List[str]) -> List[List[float]]:
    """knowledge.txt ã®å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŸ‹ã‚è¾¼ã‚€"""
    client = get_embedding_client()
    if isinstance(client, str):
        # ã‚¨ãƒ©ãƒ¼æ–‡å­—åˆ—ãŒè¿”ã£ã¦ããŸå ´åˆ
        raise RuntimeError(client)

    if not texts:
        return []

    resp = client.embeddings.create(
        model=LOCALLM_EMBEDDING_MODEL,
        input=texts,
    )
    vectors: List[List[float]] = [d.embedding for d in resp.data]
    return vectors


def embed_query(text: str) -> List[float]:
    """ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã‚€"""
    client = get_embedding_client()
    if isinstance(client, str):
        raise RuntimeError(client)

    resp = client.embeddings.create(
        model=LOCALLM_EMBEDDING_MODEL,
        input=[text],
    )
    return resp.data[0].embedding


def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    dot = 0.0
    s1 = 0.0
    s2 = 0.0
    for a, b in zip(v1, v2):
        dot += a * b
        s1 += a * a
        s2 += b * b
    if s1 == 0 or s2 == 0:
        return 0.0
    return dot / (math.sqrt(s1) * math.sqrt(s2))


@st.cache_resource(show_spinner=True)
def prepare_corpus_for_embeddings() -> Tuple[List[str], List[List[float]]]:
    """
    knowledge.txt ã‚’èª­ã¿è¾¼ã¿ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€‚
    """
    docs = get_knowledge_docs()
    if not docs:
        return [], []

    vectors = embed_texts(docs)
    return docs, vectors


def search_by_embedding(query: str, top_k: int = 3) -> Tuple[List[str], List[float]]:
    """
    åŸ‹ã‚è¾¼ã¿ã§é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
    æˆ»ã‚Šå€¤: (docs, scores)
    """
    docs, vectors = prepare_corpus_for_embeddings()
    if not docs or not vectors:
        return [], []

    q_vec = embed_query(query)
    scored: List[Tuple[float, str]] = []

    for doc, v in zip(docs, vectors):
        score = cosine_similarity(q_vec, v)
        scored.append((score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)
    top = scored[:top_k]

    top_docs = [d for _, d in top]
    top_scores = [s for s, _ in top]
    return top_docs, top_scores


# ---------------------------------------------------------
# LLM å‘¼ã³å‡ºã—ï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰
# ---------------------------------------------------------
def call_llm_with_context(query: str, contexts: List[str]) -> str:
    client = get_chat_client()
    if isinstance(client, str):
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¿”ã£ã¦ããŸå ´åˆ
        return client

    history = get_history()

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
    if contexts:
        context_text = "\n\n---\n\n".join(contexts)
    else:
        context_text = "ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆknowledge.txtï¼‰ã‹ã‚‰é–¢é€£æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    # system_prompt.txt ã®å†…å®¹ + ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã‚’çµåˆ
    base_system_prompt = load_system_prompt()
    system_content = (
        f"{base_system_prompt}\n\n"
        "-----\n"
        "ä»¥ä¸‹ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆknowledge.txt ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸé–¢é€£æƒ…å ±ï¼‰ã§ã™ã€‚"
        "å¿…è¦ã«å¿œã˜ã¦å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"{context_text}"
    )

    messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_content},
    ]

    # ç›´è¿‘ 5 ã‚¿ãƒ¼ãƒ³åˆ†ã®å±¥æ­´ã‚’è¿½åŠ 
    for turn in history[-5:]:
        messages.append({"role": "user", "content": turn["user"]})
        messages.append({"role": "assistant", "content": turn["assistant"]})

    messages.append({"role": "user", "content": query})

    resp = client.chat.completions.create(
        model=LOCALLM_CHAT_MODEL,
        messages=messages,
        temperature=0.3,
    )

    answer = resp.choices[0].message.content or ""
    return answer


# ---------------------------------------------------------
# Streamlit UI
# ---------------------------------------------------------
def main() -> None:
    st.set_page_config(
        page_title="Locallm Embedding Search",
        page_icon="ğŸ§ ",
        layout="wide",
    )
    st.title("Locallm åŸ‹ã‚è¾¼ã¿æ¤œç´¢ç‰ˆ ğŸ’¬")
    st.caption("knowledge.txt ã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã§æ¤œç´¢ã—ã¦å›ç­”ã™ã‚‹ãƒ‡ãƒ¢")

    init_session_state()

    # äº‹å‰ã«ãƒŠãƒ¬ãƒƒã‚¸èª­ã¿è¾¼ã¿ï¼ˆä»¶æ•°ã ã‘å‡ºã™ï¼‰
    docs = get_knowledge_docs()
    doc_count = len(docs)

    # -----------------------------
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    # -----------------------------
    with st.sidebar:
        # æ–°è¦ãƒãƒ£ãƒƒãƒˆ
        if st.button("æ–°è¦ãƒãƒ£ãƒƒãƒˆ", use_container_width=True):
            st.session_state.history = []
            st.session_state.messages = []
            st.session_state.loaded_log_name = None
            # session_id ã¯å†ç”Ÿæˆ
            if "session_id" in st.session_state:
                del st.session_state["session_id"]
            st.success("æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
            st.rerun()

        st.markdown("---")

        # ãƒ­ã‚°å±¥æ­´
        st.subheader("å±¥æ­´")
        log_files = list_log_files()
        if not log_files:
            st.caption("logs ãƒ•ã‚©ãƒ«ãƒ€ã«ã¾ã ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.caption("ç›´è¿‘ 20 ä»¶")
            for log_path in log_files[:20]:
                label = log_path.stem  # ä¾‹: 20251126_xxxxxxxx
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write(label)
                with col2:
                    if st.button("â†’", key=f"load_log_{label}"):
                        history = load_history_from_log(log_path)
                        st.session_state.history = history
                        # Chat UI ç”¨ messages ã‚’å†æ§‹ç¯‰
                        st.session_state.messages = []
                        for turn in history:
                            st.session_state.messages.append(
                                {"role": "user", "content": turn["user"]}
                            )
                            st.session_state.messages.append(
                                {"role": "assistant", "content": turn["assistant"]}
                            )
                        st.session_state.loaded_log_name = label
                        st.success(f"{label} ã®å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                        st.rerun()

        if st.session_state.loaded_log_name:
            st.info(f"èª­ã¿è¾¼ã¿ä¸­ã®ãƒ­ã‚°: {st.session_state.loaded_log_name}")

        st.markdown("---")

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸æ¦‚è¦
        st.header("ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸")
        st.write(f"knowledge.txt ã®æ–‡æ›¸æ•°: **{doc_count}** ä»¶")

        knowledge_path = DATA_DIR / "knowledge.txt"
        st.caption("knowledge.txt Path")
        st.code(str(knowledge_path), language="text")
        if knowledge_path.exists() and doc_count > 0:
            st.caption("knowledge.txt ç”±æ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸€ä¾‹ï¼ˆå†’é ­100æ–‡å­—ï¼‰")
            st.write(docs[0][:100])

        st.markdown("---")

        system_prompt_path = DATA_DIR / "system_prompt.txt"
        st.caption("system_prompt.txt Path")
        st.code(str(system_prompt_path), language="text")
        if system_prompt_path.exists():
            try:
                sp_text = system_prompt_path.read_text(encoding="utf-8").strip()
                if sp_text:
                    st.caption("system_prompt.txt å†’é ­100æ–‡å­—")
                    st.write(sp_text[:100])
                else:
                    st.caption("system_prompt.txt ã¯ç©ºã§ã™ã€‚")
            except Exception as e:
                st.caption(f"system_prompt.txt ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        else:
            st.caption("system_prompt.txt ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

        st.markdown("---")

        st.subheader("ç’°å¢ƒæƒ…å ±")
        st.write(f"Chat Base URL: `{LOCALLM_BASE_URL}`")
        st.write(f"Chat Model: `{LOCALLM_CHAT_MODEL}`")
        st.write(f"Embedding Base URL: `{EMB_BASE_URL or 'https://api.openai.com/v1'}`")
        st.write(f"Embedding Model: `{LOCALLM_EMBEDDING_MODEL}`")

    # -----------------------------
    # ã“ã‚Œã¾ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    # -----------------------------
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # -----------------------------
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    # -----------------------------
    query = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆknowledge.txt ã®å†…å®¹ã«é–¢ã™ã‚‹è³ªå•ãªã©ï¼‰")

    if query:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›è¡¨ç¤º
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.write(query)

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰
        with st.spinner("åŸ‹ã‚è¾¼ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™..."):
            try:
                contexts, scores = search_by_embedding(query, top_k=3)
            except RuntimeError as e:
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šç³»ã®ã‚¨ãƒ©ãƒ¼ãªã©
                error_msg = str(e)
                with st.chat_message("assistant"):
                    st.error(error_msg)
                return

        # LLM å‘¼ã³å‡ºã—
        with st.spinner("LLM ã«å•ã„åˆã‚ã›ä¸­..."):
            answer = call_llm_with_context(query, contexts)

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå›ç­”è¡¨ç¤º
        with st.chat_message("assistant"):
            st.write(answer)

            # ğŸ” ä»Šå›å‚ç…§ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ + ã‚¹ã‚³ã‚¢è¡¨ç¤º
            if contexts:
                with st.expander("ä»Šå›å‚ç…§ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆknowledge.txt, åŸ‹ã‚è¾¼ã¿æ¤œç´¢ï¼‰"):
                    for i, (ctx, sc) in enumerate(zip(contexts, scores), start=1):
                        st.markdown(f"**Doc {i} (score={sc:.3f})**")
                        st.write(ctx)
            else:
                st.caption("knowledge.txt ã‹ã‚‰é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ & ãƒ­ã‚°ä¿å­˜
        add_history(query, answer)
        try:
            log_interaction(
                question=query,
                answer=answer,
                contexts=contexts,
                extra={"scores": scores},
            )
        except Exception:
            # ãƒ­ã‚°å¤±æ•—ã§ã‚¢ãƒ—ãƒªãŒè½ã¡ãªã„ã‚ˆã†ã«
            pass


if __name__ == "__main__":
    main()
